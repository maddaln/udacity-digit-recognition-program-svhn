{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training on Google Cloud ML [Cloud-Datalab]\n",
    "\n",
    "This notebook submits a training job on [Google Cloud Machine Learning](https://cloud.google.com/ml/) and must be run inside a [Google Cloud Datalab](https://cloud.google.com/datalab/) Docker container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name svhn\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import datetime\n",
    "import traceback\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import google.cloud.ml as ml\n",
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "from tensorflow.contrib.metrics.python.ops import metric_ops\n",
    "\n",
    "img_w = 64\n",
    "img_h = 64\n",
    "SEED = 66478\n",
    "num_channels = 1\n",
    "num_labels = 6\n",
    "\n",
    "\n",
    "def log(msg):\n",
    "    tf.logging.info(msg)\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "def tf_get_length_value(length_class_tensor):\n",
    "    '''A utility function to get the length value [1,6] from the length class id [0,5]'''\n",
    "    return tf.add(length_class_tensor, 1)\n",
    "\n",
    "\n",
    "def batch_normalize(x, n_out, phase_train, scope='bn'):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                           name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                            name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed\n",
    "\n",
    "\n",
    "def get_conv2d(name, data, patch, d_in, d_out, stride=[1, 1, 1, 1], pooling_size=None, pooling_stride=None,\n",
    "               padding='SAME', batch_norm=False, is_training=True, pool_type=\"max\", keep_prob=1.0):\n",
    "    with tf.name_scope(str('%s' % name)):\n",
    "        filters = tf.Variable(tf.truncated_normal([patch, patch, d_in, d_out],\n",
    "                                                  stddev=get_conv2d_filters_init_stddev(img_w, img_h, d_in)),\n",
    "                              name=str('%s_filters' % name))\n",
    "        biases = tf.Variable(tf.zeros([d_out]), name=str('%s_b' % name))\n",
    "        layer = tf.nn.conv2d(data, filters, stride, padding=padding, name=str('%s_layers' % name))\n",
    "        layer = layer + biases\n",
    "        layer = tf.nn.relu(layer)\n",
    "        if batch_norm:\n",
    "            layer = batch_normalize((layer), d_out, tf.convert_to_tensor(is_training, dtype=tf.bool),\n",
    "                                    str('%s_bn' % name))\n",
    "        if pooling_stride is not None and pooling_size is not None:\n",
    "            if pool_type == \"max\":\n",
    "                layer = tf.nn.max_pool(layer, pooling_size, pooling_stride, padding=padding)\n",
    "            else:\n",
    "                layer = tf.nn.avg_pool(layer, pooling_size, pooling_stride, padding=padding)\n",
    "        if keep_prob < 1.0:\n",
    "            layer = tf.nn.dropout(layer, keep_prob=keep_prob)\n",
    "    return filters, biases, layer\n",
    "\n",
    "\n",
    "def get_fc(name, data, depth, relu=True, keep_prob=1):\n",
    "    with tf.name_scope(str('%s' % name)):\n",
    "        inbound = int(data.get_shape()[1])\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([inbound, depth], stddev=math.sqrt(2.0 / inbound), name=str('%s_w' % name)))\n",
    "        biases = tf.Variable(tf.zeros([depth]), name=str('%s_b' % name))\n",
    "        layer = tf.matmul(data, weights) + biases\n",
    "        if relu is True:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        if keep_prob < 1:\n",
    "            layer = tf.nn.dropout(layer, keep_prob=keep_prob, seed=SEED)\n",
    "        return weights, biases, layer\n",
    "\n",
    "\n",
    "def get_conv2d_filters_init_stddev(w, h, d_in):\n",
    "    # from https://arxiv.org/pdf/1502.01852v1.pdf\n",
    "    return math.sqrt(2.0 / (w * h * d_in))\n",
    "\n",
    "\n",
    "def conv_to_fc(conv):\n",
    "    shape = conv.get_shape().as_list()\n",
    "    return tf.reshape(conv, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "\n",
    "def get_features(data, keep_prob=0.7, is_training=True):\n",
    "    with tf.name_scope('layers'):\n",
    "        # Model.\n",
    "        w1, b1, conv1 = get_conv2d('conv1', data=data, patch=5, d_in=3, d_out=48,\n",
    "                                   stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                   padding='SAME', batch_norm=True, is_training=is_training)\n",
    "\n",
    "        w2, b2, conv2 = get_conv2d('conv2', data=conv1, patch=5, d_in=48, d_out=64,\n",
    "                                   stride=[1, 1, 1, 1], padding='SAME', batch_norm=True, is_training=is_training)\n",
    "\n",
    "        w3, b3, conv3 = get_conv2d('conv3', data=conv2, patch=5, d_in=64, d_out=128,\n",
    "                                   stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                   padding='SAME',\n",
    "                                   batch_norm=True, is_training=is_training, keep_prob=keep_prob)\n",
    "\n",
    "        w4, b4, conv4 = get_conv2d('conv4', data=conv3, patch=5, d_in=128, d_out=160,\n",
    "                                   stride=[1, 1, 1, 1], padding='SAME', batch_norm=True, is_training=is_training)\n",
    "\n",
    "        w5, b5, conv5 = get_conv2d('conv5', data=conv4, patch=5, d_in=160, d_out=192,\n",
    "                                   stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                   padding='SAME',\n",
    "                                   batch_norm=True, is_training=is_training, keep_prob=keep_prob)\n",
    "\n",
    "        w6, b6, conv6 = get_conv2d('conv6', data=conv5, patch=3, d_in=192, d_out=192,\n",
    "                                   stride=[1, 1, 1, 1], padding='SAME', batch_norm=True, is_training=is_training)\n",
    "\n",
    "        w7, b7, conv7 = get_conv2d('conv7', data=conv6, patch=3, d_in=192, d_out=192,\n",
    "                                   stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                   padding='SAME',\n",
    "                                   batch_norm=True, is_training=is_training, pool_type=\"avg\", keep_prob=keep_prob)\n",
    "\n",
    "        w8, b8, conv8 = get_conv2d('conv8', data=conv7, patch=3, d_in=192, d_out=192,\n",
    "                                   stride=[1, 1, 1, 1], padding='SAME', batch_norm=True, is_training=is_training)\n",
    "\n",
    "        w9, b9, conv9 = get_conv2d('conv9', data=conv8, patch=3, d_in=192, d_out=384,\n",
    "                                   stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                   padding='SAME',\n",
    "                                   batch_norm=True, is_training=is_training, pool_type=\"avg\", keep_prob=keep_prob)\n",
    "\n",
    "        w10, b10, conv10 = get_conv2d('conv10', data=conv9, patch=2, d_in=384, d_out=768,\n",
    "                                      stride=[1, 1, 1, 1], pooling_size=[1, 2, 2, 1], pooling_stride=[1, 2, 2, 1],\n",
    "                                      padding='SAME',\n",
    "                                      batch_norm=True, is_training=is_training, pool_type=\"avg\", keep_prob=keep_prob)\n",
    "\n",
    "        reg_vars = [w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10]\n",
    "        regularizers = 0;\n",
    "        for val in reg_vars:\n",
    "            regularizers += tf.nn.l2_loss(val)\n",
    "        return conv_to_fc(conv10), regularizers\n",
    "\n",
    "\n",
    "def get_logits(features):\n",
    "    with tf.name_scope('logits'):\n",
    "        # Length logits and weights\n",
    "        length_weigths, length_biases, logits_length = get_fc('logits_L', features, num_labels, relu=False)\n",
    "\n",
    "        # Digits logits and weights\n",
    "        digits_pack = [get_fc(str(\"logits_D%d\" % i), features, 10, relu=False) for i in range(num_labels)]\n",
    "        logits_digits = tf.pack([digits_pack[i][2] for i in range(num_labels)])\n",
    "        return logits_length, logits_digits\n",
    "\n",
    "\n",
    "def get_loss(logits_length, logits_digits, labels_length, digit_labels_t, l2, batch_size, regularizers):\n",
    "    with tf.name_scope('loss'):\n",
    "        # Loss calculation\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_length, labels_length))\n",
    "\n",
    "        # Add the regularization term to the loss.\n",
    "        loss += l2 * regularizers\n",
    "        # generating a tensor (?,num_labels) where each row contains 1s, 2s, 3s,... Will be used\n",
    "\n",
    "        fills = tf.pack([tf.fill([batch_size], i) for i in range(num_labels)])\n",
    "\n",
    "        masks = list()\n",
    "        dmasked = list()\n",
    "        lmasked = list()\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            # get a mask of logits for the first (length - 1) digits\n",
    "            # (?,6) => (count(digit_index < length), 6)\n",
    "            masks.append(tf.less(fills[i], tf_get_length_value(labels_length)))\n",
    "            dmasked.append(tf.boolean_mask(logits_digits[i], masks[i]))\n",
    "            lmasked.append(tf.boolean_mask(digit_labels_t[i], masks[i]))\n",
    "            loss += tf.cond(tf.less(0, tf.shape(lmasked[i])[0]),\n",
    "                            lambda: tf.reduce_mean(\n",
    "                                tf.nn.sparse_softmax_cross_entropy_with_logits(dmasked[i], lmasked[i])),\n",
    "                            lambda: tf.constant(0.0))\n",
    "\n",
    "        masks = tf.pack(masks)\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "        return loss, masks, lmasked, dmasked\n",
    "\n",
    "\n",
    "def train(learning_rate, loss):\n",
    "    with tf.name_scope('train_op'):\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        #         learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step, 7000, 0.0001, power=0.5)\n",
    "        tf.scalar_summary(\"learning rate\", learning_rate)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(loss,\n",
    "                                                                                             global_step=global_step)\n",
    "        #         train_op = tf.train.RMSPropOptimizer(starter_learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=True, name='RMSProp').minimize(loss,global_step=global_step)\n",
    "        return global_step, train_op\n",
    "\n",
    "\n",
    "def preds_and_accuracy(logits_length, logits_digits, masks, lmasked, dmasked, labels_length, digit_labels_t):\n",
    "    # length prediction\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds_l = tf.nn.softmax(logits_length)\n",
    "        # digits predictions (including non existing digits)\n",
    "        preds_d = [tf.nn.softmax(logits_digits[i]) for i in range(num_labels)]\n",
    "\n",
    "    ### ACCURACY REPORTING ###\n",
    "    with tf.name_scope('correct'):\n",
    "        # Compute a (i=num_labels,j=?) tensor where True means \n",
    "        # that the ith digit from the jth number was correctly predicted.\n",
    "        correct_preds_d = tf.pack(\n",
    "            [tf.equal(tf.cast(tf.argmax(preds_d[i], 1), tf.int32), digit_labels_t[i]) for i in range(num_labels)])\n",
    "\n",
    "        # Derive a (?,num_labels) binary matrix where:\n",
    "        # 1 means the j-th digit from i-th number was predicted correctly and\n",
    "        # 0 means the digit is either not predicted correctly or not present (if the digit position >= number length)\n",
    "        correctness_matrix = tf.transpose(tf.mul(tf.cast(correct_preds_d, tf.int32), tf.cast(masks, tf.int32)))\n",
    "\n",
    "        # The global prediction is correct if the sum along a given row equals the length of this row's number.\n",
    "        correct = tf.equal(tf.reduce_sum(correctness_matrix, 1), tf.cast(tf_get_length_value(labels_length), tf.int32))\n",
    "        correct_l = tf.equal(tf.cast(tf.argmax(preds_l, 1), tf.int32), labels_length)\n",
    "\n",
    "        # Remove predictions of non existent digits: (ex: prediction for digit 4 when there are only 3 digits)\n",
    "        # Doing so allows to compute accurate accuracy for each digit.\n",
    "        preds_d_masked = [tf.nn.softmax(tf.boolean_mask(logits_digits[i], masks[i])) for i in range(num_labels)]\n",
    "        correct_d = [tf.equal(tf.cast(tf.argmax(preds_d_masked[i], 1), tf.int32), lmasked[i]) for i in\n",
    "                     range(num_labels)]\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) * 100\n",
    "        # Length prediction accuracy       \n",
    "        accuracy_l = tf.reduce_mean(tf.cast(correct_l, tf.float32)) * 100\n",
    "        # Individual digits predictions accuracy\n",
    "        accuracy_d = tf.pack([tf.reduce_mean(tf.cast(correct_d[i], tf.float32)) * 100 for i in range(num_labels)])\n",
    "        # create a summary for our cost and accuracy\n",
    "        tf.scalar_summary(\"accuracy\", accuracy)\n",
    "        tf.scalar_summary(\"accuracy length\", accuracy_l)\n",
    "        for i in range(6):\n",
    "            tf.scalar_summary((\"accuracy digit %d\" % i), accuracy_d[i])\n",
    "\n",
    "    return preds_l, preds_d, accuracy_l, accuracy_d, accuracy, correct\n",
    "\n",
    "\n",
    "def read_and_decode_single_example(filenames, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(filenames, num_epochs=None)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'labels_value': tf.FixedLenFeature([], tf.int64),\n",
    "            'labels_length': tf.FixedLenFeature([], tf.int64),\n",
    "            'labels_digits': tf.FixedLenFeature([6], tf.int64),\n",
    "            'data': tf.FixedLenFeature([64, 64, 3], tf.float32),\n",
    "        })\n",
    "    return features['data'], features['labels_digits'], features['labels_length'], features['labels_value']\n",
    "\n",
    "\n",
    "def get_inputs(files, batch_size=16, num_epochs=None, is_training=True):\n",
    "    thread_count = multiprocessing.cpu_count()\n",
    "    # The minimum number of instances in a queue from which examples are drawn\n",
    "    # randomly. The larger this number, the more randomness at the expense of\n",
    "    # higher memory requirements.\n",
    "    MIN_AFTER_DEQUEUE = 100\n",
    "\n",
    "    # When batching data, the queue's capacity will be larger than the batch_size\n",
    "    # by some factor. The recommended formula is (num_threads + a small safety\n",
    "    # margin). For now, we use a single thread for reading, so this can be small.\n",
    "    QUEUE_SIZE_MULTIPLIER = thread_count + 3\n",
    "    capacity = MIN_AFTER_DEQUEUE + QUEUE_SIZE_MULTIPLIER * batch_size\n",
    "    # input images\n",
    "    with tf.name_scope('input'):\n",
    "        # get single examples\n",
    "        image, digits, length, value = read_and_decode_single_example(files, num_epochs)\n",
    "        # groups examples into batches randomly\n",
    "        data = None\n",
    "        digit_labels = None\n",
    "        labels_length = None\n",
    "        labels_value = None\n",
    "\n",
    "        if is_training:\n",
    "            data, digit_labels, labels_length, labels_value = tf.train.shuffle_batch([image, digits, length, value],\n",
    "                                                                                     batch_size=batch_size,\n",
    "                                                                                     capacity=capacity,\n",
    "                                                                                     min_after_dequeue=MIN_AFTER_DEQUEUE,\n",
    "                                                                                     num_threads=thread_count)\n",
    "        else:\n",
    "            data, digit_labels, labels_length, labels_value = tf.train.batch([image, digits, length, value],\n",
    "                                                                             batch_size=batch_size,\n",
    "                                                                             capacity=capacity,\n",
    "                                                                             num_threads=thread_count)\n",
    "\n",
    "        labels_length = tf.cast(labels_length, tf.int32, name=\"cast_labels_lengths\")\n",
    "        labels_value = tf.cast(labels_value, tf.int32, name=\"cast_labels_value\")\n",
    "        digit_labels = tf.transpose(tf.cast(digit_labels, tf.int32, name=\"cast_digit_labels\"),\n",
    "                                    name=\"transpose_digit_labels\")\n",
    "        log(data.get_shape())\n",
    "        return data, digit_labels, labels_length, labels_value;\n",
    "\n",
    "\n",
    "def evaluate(args, trial_id, glob_step, test=False):\n",
    "    with tf.Graph().as_default() as graph_eval:\n",
    "        log(\"Running eval on %s set.\" % ('test' if test else 'valid'))\n",
    "        paths = args.test_data_paths if test else args.eval_data_paths\n",
    "        dataset_size = args.test_dataset_size if test else args.eval_dataset_size\n",
    "        data, digit_labels_t, labels_length, labels_value = get_inputs(paths,\n",
    "                                                                       batch_size=args.eval_batch_size, num_epochs=None,\n",
    "                                                                       is_training=False)\n",
    "        # get features\n",
    "        features, regularizers = get_features(data, keep_prob=1, is_training=False)\n",
    "        # get logits\n",
    "        logits_length, logits_digits = get_logits(features)\n",
    "        # get loss\n",
    "        loss, masks, lmasked, dmasked = get_loss(logits_length, logits_digits, labels_length, digit_labels_t,\n",
    "                                                 args.l2_beta, args.eval_batch_size, regularizers)\n",
    "        # inference and accuracy\n",
    "        preds_l, preds_d, accuracy_l, accuracy_d, accuracy, correct = preds_and_accuracy(logits_length, logits_digits,\n",
    "                                                                                         masks, lmasked, dmasked,\n",
    "                                                                                         labels_length, digit_labels_t)\n",
    "        # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "        summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = None\n",
    "        \n",
    "        subdir = 'eval' if not test else 'test'\n",
    "        log('sub %s' % subdir)\n",
    "        summary_writer = tf.train.SummaryWriter(os.path.join(args.summaries_path, subdir), graph=graph_eval)\n",
    "        sv = tf.train.Supervisor(graph=graph_eval,\n",
    "                                 logdir=os.path.join(args.checkpoints_dir, subdir),\n",
    "                                 summary_op=None,\n",
    "                                 summary_writer=summary_writer,\n",
    "                                 global_step=None,\n",
    "                                 save_model_secs=0,\n",
    "                                 saver=saver)\n",
    "\n",
    "        eval_qty = 0\n",
    "        acc_d = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        acc_l = 0.0\n",
    "        acc = 0.0\n",
    "        with sv.managed_session(master=\"\", start_standard_services=False) as sess_eval:\n",
    "            last_checkpoint = tf.train.latest_checkpoint(os.path.join(args.checkpoints_dir, 'train'))\n",
    "            if last_checkpoint is None:\n",
    "                log(\"No checkpoint available for model evaluation.\")\n",
    "                return\n",
    "            sv.saver.restore(sess_eval, last_checkpoint)\n",
    "            sv.start_queue_runners(sess_eval)\n",
    "            while not sv.should_stop() and eval_qty < dataset_size:\n",
    "                vpreds_d, vpreds_l, vacc_d, vacc_l, vcorr, vacc, values = sess_eval.run(\n",
    "                    [preds_d, preds_l, accuracy_d, accuracy_l, correct, accuracy, labels_value])\n",
    "                acc += vacc\n",
    "                acc_l += vacc_l\n",
    "                for i in range(len(vacc_d)):\n",
    "                    acc_d[i] += vacc_d[i]\n",
    "#                 if glob_step >= 0:\n",
    "#                     log(\"Eval batch:%d - len:%d \" % (eval_qty / args.eval_batch_size, len(vpreds_d[0])))\n",
    "#                     log(\"Valid. acc: %.1f%% len: %.1f%% d0: %.1f%% d1: %.1f%% d2: %.1f%% d3: %.1f%% d4: %.1f%% d5: %.1f%%\"\n",
    "#                         % (vacc, vacc_l, vacc_d[0], vacc_d[1], vacc_d[2], vacc_d[3], vacc_d[4], vacc_d[5]))\n",
    "                eval_qty += args.eval_batch_size\n",
    "                \n",
    "            # write summary when all batches are evaluated\n",
    "            num_steps = eval_qty / args.eval_batch_size\n",
    "            acc /= num_steps\n",
    "            acc_l /= num_steps\n",
    "            acc_d /= num_steps\n",
    "\n",
    "            log(\"Valid. acc: %.1f%% len: %.1f%% d0: %.1f%% d1: %.1f%% d2: %.1f%% d3: %.1f%% d4: %.1f%% d5: %.1f%%\"\n",
    "                % (acc, acc_l, acc_d[0], acc_d[1], acc_d[2], acc_d[3], acc_d[4], acc_d[5]))\n",
    "            summary = tf.Summary(value=[\n",
    "                tf.Summary.Value(tag=\"accuracy\", simple_value=acc),\n",
    "                tf.Summary.Value(tag=\"accuracy length\", simple_value=acc_l),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 0\", simple_value=acc_d[0]),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 1\", simple_value=acc_d[1]),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 2\", simple_value=acc_d[2]),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 3\", simple_value=acc_d[3]),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 4\", simple_value=acc_d[4]),\n",
    "                tf.Summary.Value(tag=\"accuracy digit 5\", simple_value=acc_d[5]),\n",
    "            ])\n",
    "            summary_writer.add_summary(summary, max(0,glob_step))\n",
    "        log(\"eval finished\")\n",
    "\n",
    "def get_length_from_class(class_id):\n",
    "    return class_id + 1\n",
    "\n",
    "\n",
    "def run_training(args, target, is_chief, device_fn, trial_id):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(device_fn):\n",
    "\n",
    "            # get inputs\n",
    "            data, digit_labels_t, labels_length, _ = get_inputs(args.train_data_paths, batch_size=args.batch_size,\n",
    "                                                                num_epochs=args.num_epochs)\n",
    "            # get features\n",
    "            features, regularizers = get_features(data, keep_prob=args.keep_prob, is_training=True)\n",
    "            # get logits\n",
    "            logits_length, logits_digits = get_logits(features)\n",
    "            # get loss\n",
    "            loss, masks, lmasked, dmasked = get_loss(logits_length, logits_digits, labels_length, digit_labels_t,\n",
    "                                                     args.l2_beta, args.batch_size, regularizers)\n",
    "            # Train op\n",
    "            global_step, train_op = train(args.learning_rate, loss)\n",
    "\n",
    "            # inference and accuracy\n",
    "            preds_l, preds_d, accuracy_l, accuracy_d, accuracy, correct = preds_and_accuracy(logits_length,\n",
    "                                                                                             logits_digits, masks,\n",
    "                                                                                             lmasked, dmasked,\n",
    "                                                                                             labels_length,\n",
    "                                                                                             digit_labels_t)\n",
    "\n",
    "            # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "            summary_op = tf.merge_all_summaries()\n",
    "\n",
    "            glob_step = 0\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            summary_writer = tf.train.SummaryWriter(os.path.join(args.summaries_path, 'train'), graph=graph)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # Create a \"supervisor\", which oversees the training process.\n",
    "            sv = tf.train.Supervisor(is_chief=is_chief,\n",
    "                                     logdir=os.path.join(args.checkpoints_dir, 'train'),\n",
    "                                     init_op=init_op,\n",
    "                                     saver=saver,\n",
    "                                     summary_writer=summary_writer,\n",
    "                                     summary_op=summary_op,\n",
    "                                     global_step=global_step,\n",
    "                                     save_summaries_secs=args.save_summaries_secs,\n",
    "                                     save_model_secs=args.save_model_secs)\n",
    "\n",
    "            with sv.managed_session(target) as sess:\n",
    "                sess_start_time = time.time()\n",
    "                eval_counter = time.time()\n",
    "                while not sv.should_stop() and glob_step <= args.max_steps:\n",
    "\n",
    "                    _, loss_value, batch_accuracy, glob_step = sess.run([train_op, loss, accuracy, global_step])\n",
    "\n",
    "                    # Write the summaries and log an overview fairly often.\n",
    "                    if glob_step % 100 == 0:\n",
    "                        log(\"Step %d: loss = %.2f (%.3f sec)\" % (glob_step, loss_value, time.time() - sess_start_time))\n",
    "\n",
    "                    if time.time() - eval_counter > args.eval_model_secs and is_chief:\n",
    "                        start_time2 = time.time()\n",
    "                        evaluate(args, trial_id, glob_step)\n",
    "                        log(\"Step %d: saving and reporting took %.3f sec\" % (glob_step, time.time() - start_time2))\n",
    "                        eval_counter = time.time()\n",
    "\n",
    "                if is_chief:\n",
    "                    # Force a save at the end of our loop.\n",
    "                    log(\"Saving before closing\")\n",
    "                    sv.saver.save(sess, sv.save_path, global_step=global_step, write_meta_graph=False)\n",
    "                    evaluate(args, trial_id, args.max_steps, test=True)\n",
    "\n",
    "                    # Save the model for inference\n",
    "    # export_model(args, sess, sv.saver, trial_id)\n",
    "\n",
    "    # Ask for all the services to stop.\n",
    "    sv.stop()\n",
    "    tf.logging.info(\"Done training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha module --name task --main\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from . import svhn\n",
    "import google.cloud.ml as ml\n",
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "def start_server(cluster, task):\n",
    "  # Create and start a server.\n",
    "  return tf.train.Server(cluster,\n",
    "                         protocol=\"grpc\",\n",
    "                         job_name=task['type'],\n",
    "                         task_index=task['index'])\n",
    "\n",
    "def dispatch(args, cluster, task, job, trial_id):\n",
    "  \n",
    "  if not cluster:\n",
    "    # Run locally.\n",
    "    if args.train_data_paths is None or len(args.train_data_paths) == 0:\n",
    "      svhn.evaluate(args, trial_id, -1, test=True)\n",
    "    else:\n",
    "      svhn.run_training(args, target=\"\", is_chief=True, device_fn=\"\", trial_id=trial_id)\n",
    "    return\n",
    "\n",
    "  if task['type'] == \"ps\":\n",
    "    server = start_server(cluster, task)\n",
    "    server.join()\n",
    "  elif task['type'] == \"worker\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = False\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    if args.train_data_paths is None or len(args.train_data_paths) == 0:\n",
    "      svhn.evaluate(args, trial_id, -1, test=True)\n",
    "    else:\n",
    "      svhn.run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  elif task['type'] == \"master\":\n",
    "    server = start_server(cluster, task)\n",
    "    is_chief = (task['index'] == 0)\n",
    "    device_fn = tf.train.replica_device_setter(\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:master/task:%d\" % task['index'],\n",
    "        cluster=cluster)\n",
    "    if args.train_data_paths is None or len(args.train_data_paths) == 0:\n",
    "      svhn.evaluate(args, trial_id, -1, test=True)\n",
    "    else:\n",
    "      svhn.run_training(args, server.target, is_chief, device_fn, trial_id)\n",
    "  else:\n",
    "    raise ValueError(\"invalid job_type %s\" % task['type'])\n",
    "    \n",
    "\n",
    "def parse_arguments():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--train_data_paths', type=str, action='append')\n",
    "  parser.add_argument('--eval_data_paths', type=str, action='append')\n",
    "  parser.add_argument('--test_data_paths', type=str, action='append')\n",
    "  parser.add_argument('--metadata_path', type=str)\n",
    "  parser.add_argument('--output_path', type=str)\n",
    "  parser.add_argument('--max_steps', type=int, default=5000)\n",
    "  parser.add_argument('--verbose', type=bool, default=True)\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "  parser.add_argument('--l2_beta', type=float, default=0.001)\n",
    "  parser.add_argument('--batch_size', type=int, default=32)\n",
    "  parser.add_argument('--eval_batch_size', type=int, default=128)\n",
    "  parser.add_argument('--summaries_path', type=str)\n",
    "  parser.add_argument('--checkpoints_dir', type=str)\n",
    "  parser.add_argument('--num_epochs', type=int)\n",
    "  parser.add_argument('--keep_prob', type=float, default=0.5)\n",
    "  parser.add_argument('--save_model_secs', type=int, default=120)\n",
    "  parser.add_argument('--save_summaries_secs', type=int, default=120)\n",
    "  parser.add_argument('--eval_model_secs', type=int, default=600)\n",
    "  parser.add_argument('--eval_dataset_size', type=int, default=0)\n",
    "  parser.add_argument('--test_dataset_size', type=int, default=0)\n",
    "  return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "  config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  cluster = config.get('cluster', None)\n",
    "  task = config.get('task', None)\n",
    "  job = config.get('job', None)\n",
    "  # First find out if there's a task value on the environment variable.\n",
    "  # If there is none or it is empty define a default one.\n",
    "  task_data = config.get('task', None) or {'type': 'master', 'index': 0}\n",
    "  args = parse_arguments()\n",
    "  trial_id = task_data.get('trial')\n",
    "  if trial_id is not None:\n",
    "    output_dir = os.path.join(args.checkpoints_dir, trial_id)\n",
    "  else:\n",
    "    output_dir = args.checkpoints_dir\n",
    "  dispatch(args, cluster, task, job, trial_id)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths variables for local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# project_dir = \"/content/git/TensorFlow-Udacity-Course/final_project/final\"\n",
    "project_dir = \"/content/git/tensorflow/final_project/final\"\n",
    "package_local_dir = os.path.join(project_dir, 'tmp', 'distributed-rgb')\n",
    "package_path_local = os.path.join(package_local_dir, \"trainer-0.1.tar.gz\")\n",
    "\n",
    "train_paths_local = [str(os.path.join(project_dir , 'data', 'tfrecords', 'rgb','train', i)) \n",
    "                    for i in os.listdir(os.path.join(project_dir , 'data', 'tfrecords',  'rgb', 'train')) if re.match(r'.*\\.tfrecords', i)]\n",
    "eval_paths_local =[str(os.path.join(project_dir , 'data', 'tfrecords', 'rgb','valid', i)) \n",
    "                    for i in os.listdir(os.path.join(project_dir , 'data', 'tfrecords',  'rgb', 'valid')) if re.match(r'.*\\.tfrecords', i)]\n",
    "test_paths_local =[str(os.path.join(project_dir , 'data', 'tfrecords', 'rgb','test', i)) \n",
    "                    for i in os.listdir(os.path.join(project_dir , 'data', 'tfrecords',  'rgb', 'test')) if re.match(r'.*\\.tfrecords', i)]\n",
    "cropped_test_paths_local =[str(os.path.join(project_dir , 'data', 'tfrecords', 'rgb','test-cropped', i)) \n",
    "                    for i in os.listdir(os.path.join(project_dir , 'data', 'tfrecords',  'rgb', 'test-cropped')) if re.match(r'.*\\.tfrecords', i)]\n",
    "\n",
    "checkpoints_dir_local = os.path.join(project_dir, 'output','checkpoints')\n",
    "summaries_path_local = os.path.join(project_dir, 'output','summaries')\n",
    "\n",
    "if not os.path.exists(package_local_dir):\n",
    "  os.makedirs(package_local_dir)\n",
    "if not os.path.exists(summaries_path_local):\n",
    "  os.makedirs(summaries_path_local)\n",
    "if not os.path.exists(checkpoints_dir_local):\n",
    "  os.makedirs(checkpoints_dir_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the package as a tar ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package created at /content/git/tensorflow/final_project/final/tmp/distributed-rgb/trainer-0.1.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "%%mlalpha package --out $package_local_dir --name trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training job locally with the parameters defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%mlalpha train\n",
    "package_uris : $package_path_local\n",
    "python_module: trainer.task\n",
    "parameter_server_count: 1\n",
    "worker_count: 1\n",
    "args:\n",
    "  train_data_paths : $train_paths_local\n",
    "  eval_data_paths :  $eval_paths_local \n",
    "  test_data_paths :  $cropped_test_paths_local   \n",
    "  checkpoints_dir : $checkpoints_dir_local  \n",
    "  summaries_path: $summaries_path_local   \n",
    "  learning_rate: 0.0001\n",
    "  l2_beta: 16e-4\n",
    "  max_steps : 30\n",
    "  batch_size : 32\n",
    "  keep_prob : 0.65\n",
    "  eval_dataset_size : 3335\n",
    "  eval_batch_size : 110\n",
    "  save_summaries_sec : 30\n",
    "  save_model_secs : 60\n",
    "  eval_model_secs : 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an evaluation pass locally. If no training paths are passed as arguments, the program runs an evaluation on the test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job Running...</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/_nocachecontent/master\" target=\"_blank\">master log</a>&nbsp;&nbsp;"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "master: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job master -> {0 -> localhost:52438}<br/>master: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:52438<br/>master: INFO:tensorflow:Running eval on test set.<br/>master: INFO:tensorflow:(16, 64, 64, 3)<br/>master: <br/>master: INFO:tensorflow:sub test<br/>master: <br/>master: INFO:tensorflow:Valid. acc: 5.0% len: 26.2% d0: 17.5% d1: 10.0% d2: 13.3% d3: nan% d4: nan% d5: nan%<br/>master: INFO:tensorflow:eval finished<br/>master: Running eval on test set.<br/>master: (16, 64, 64, 3)<br/>master: sub test<br/>master: Valid. acc: 5.0% len: 26.2% d0: 17.5% d1: 10.0% d2: 13.3% d3: nan% d4: nan% d5: nan%<br/>master: eval finished<br/>master: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Job Finished.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mlalpha train\n",
    "package_uris : $package_path_local\n",
    "python_module: trainer.task\n",
    "args:\n",
    "  test_data_paths :  $cropped_test_paths_local   \n",
    "  checkpoints_dir : $checkpoints_dir_local  \n",
    "  summaries_path: $summaries_path_local  \n",
    "  eval_batch_size : 16\n",
    "  test_dataset_size : 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the cloud with Google Cloud ML\n",
    "- install gcloud from https://cloud.google.com/sdk/\n",
    "- login with the command `gcloud auth login`\n",
    "- set your current Google Cloud project with `gcloud config set project PROJECT_ID`\n",
    "- to use TensorBoard you must provide Google Cloud Storage credentials with: `gcloud auth application-default login`\n",
    "- Make sure Cloud ML APIs are enabled:\n",
    "https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow,compute_component,logging,storage_component,storage_api,bigquery\n",
    "- Then grant your model access to your bucket. Change BUCKET_NAME and paste the instructions below in your shell:\n",
    "\n",
    "        BUCKET_NAME=\"svhn-data\"\n",
    "        PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "  AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "        SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "                -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "                    https://ml.googleapis.com/v1beta1/projects/${PROJECT_ID}:getConfig \\\n",
    "                | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "                print response['serviceAccount']\")\n",
    "  gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET_NAME\n",
    "  gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET_NAME\n",
    "  gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_file_max = 619 # the number of training .tfrecords files\n",
    "valid_file_max = 3\n",
    "test_file_max = 13\n",
    "bucket = 'gs://' + 'svhn-data'\n",
    "package_dir = os.path.join(bucket, 'model')\n",
    "package_path_remote = os.path.join(package_dir, 'trainer-regularized-3.tar.gz')\n",
    "train_paths_remote = [str(os.path.join(bucket, 'data', 'rgb', 'train', 'svhn-64x64-rgb-train.%d.tfrecords' % i)) for i in range (train_file_max+1)]\n",
    "eval_paths_remote = [str(os.path.join(bucket, 'data', 'rgb', 'valid', 'svhn-64x64-rgb-valid.%d.tfrecords' % i)) for i in range (valid_file_max+1)]\n",
    "checkpoints_dir_remote = os.path.join(bucket, 'output','checkpoints-regmodel-3')\n",
    "checkpoints_dir_export = os.path.join(project_dir, 'output','export','checkpoints-regmodel-3')\n",
    "summaries_path_remote = os.path.join(bucket, 'output','summaries-regmodel-3')\n",
    "summaries_path_export = os.path.join(project_dir, 'output','export','summaries-regmodel-3')\n",
    "test_paths_remote = [str(os.path.join(bucket, 'data', 'rgb','test', 'svhn-64x64-rgb-test.%d.tfrecords' % i)) for i in range (test_file_max+1)]\n",
    "test_cropped_paths_remote = [str(os.path.join(bucket, 'data', 'rgb', 'test-cropped', 'svhn-64x64-rgb-test.%d.tfrecords' % i)) for i in range (test_file_max+1)]\n",
    "if not os.path.exists(checkpoints_dir_export):\n",
    "  os.makedirs(checkpoints_dir_export)\n",
    "if not os.path.exists(summaries_path_export):\n",
    "  os.makedirs(summaries_path_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the package exported above to the Google Cloud Storage location defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/git/tensorflow/final_project/final/tmp/distributed-rgb/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "- [1 files][  7.5 KiB/  7.5 KiB]                                                \n",
      "Operation completed over 1 objects/7.5 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $package_path_local $package_path_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean outputs from previous jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !gsutil -m rm -r $summaries_path_remote\n",
    "# !gsutil -m rm -r $checkpoints_dir_remote/*.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training job in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_161216_220409\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_161216_220409\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=svhn-udacity&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_161216_220409\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_path_remote\n",
    "python_module: trainer.task\n",
    "scale_tier: STANDARD_1\n",
    "region: us-central1\n",
    "args:\n",
    "  train_data_paths : $train_paths_remote\n",
    "  eval_data_paths :  $eval_paths_remote \n",
    "  test_data_paths : $test_paths_remote\n",
    "  checkpoints_dir : $checkpoints_dir_remote  \n",
    "  summaries_path: $summaries_path_remote\n",
    "  learning_rate: 0.0001\n",
    "  l2_beta: 6e-4\n",
    "  max_steps : 200500\n",
    "  batch_size : 32\n",
    "  eval_dataset_size : 3300\n",
    "  test_dataset_size : 13000\n",
    "  eval_batch_size : 110  \n",
    "  save_summaries_sec : 120\n",
    "  save_model_secs : 120\n",
    "  eval_model_secs : 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out Tensorboard command line instruction\n",
    "Authorize Tensorboard with `gcloud auth application-default login`before launching it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard start --logdir=train:gs://svhn-data/output/summaries-regmodel-3/train,eval:gs://svhn-data/output/summaries-regmodel-3/eval,test:gs://svhn-data/output/summaries-regmodel-3/test\n"
     ]
    }
   ],
   "source": [
    "print (\"tensorboard start --logdir=train:%s,eval:%s,test:%s\" % (os.path.join(summaries_path_remote,\"train\"),os.path.join(summaries_path_remote,\"eval\"),os.path.join(summaries_path_remote,\"test\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Backup first locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!gsutil -m rsync -d -r $summaries_path_remote $summaries_path_export\n",
    "!gsutil -m rsync -d -r $checkpoints_dir_remote $checkpoints_dir_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an evaluation on the test set by omitting training data in arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Job \"trainer_task_161218_030920\" was submitted successfully.<br/>Run \"%mlalpha jobs --name trainer_task_161218_030920\" to view the status of the job.</p><p>Click <a href=\"https://console.developers.google.com/logs/viewer?project=svhn-udacity&resource=ml.googleapis.com%2Fjob_id%2Ftrainer_task_161218_030920\" target=\"_blank\">here</a> to view cloud log. <br/>Start TensorBoard by running \"%tensorboard start --logdir=&lt;YourLogDir&gt;\".</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mlalpha train --cloud\n",
    "package_uris: $package_path_remote\n",
    "python_module: trainer.task\n",
    "scale_tier: BASIC\n",
    "region: us-central1\n",
    "args:\n",
    "  test_data_paths : $test_cropped_paths_remote \n",
    "  checkpoints_dir : $checkpoints_dir_remote\n",
    "  summaries_path: $summaries_path_remote\n",
    "  eval_batch_size : 130\n",
    "  test_dataset_size : 13000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
